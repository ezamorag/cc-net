{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent receives visual inputs (165x220\n",
    "RGB pixels) and language inputs (an example input is shown\n",
    "in Appendix Fig. 9). The pixel inputs pass through a series\n",
    "of four ResNet blocks, with 3×3 kernels, strides of 2, 2, 2,\n",
    "2, and an increasing number of output channels (32, 128,\n",
    "256, 512). This results in 14×11 feature vectors, which we\n",
    "flatten into a list of 154 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# paramters = 4791680\n",
      "output size = torch.Size([1, 512, 154])\n",
      "ResNet(\n",
      "  (layer1): ResBlock(\n",
      "    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential(\n",
      "      (0): Conv2d(3, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): ResBlock(\n",
      "    (conv1): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential(\n",
      "      (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): ResBlock(\n",
      "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): ResBlock(\n",
      "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.shortcut = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                                      nn.BatchNorm2d(out_channels),\n",
    "                                      )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.layer1 = ResBlock(in_channels=3,   out_channels=32,  stride=2)  \n",
    "        self.layer2 = ResBlock(in_channels=32,  out_channels=128, stride=2)  \n",
    "        self.layer3 = ResBlock(in_channels=128, out_channels=256, stride=2)  \n",
    "        self.layer4 = ResBlock(in_channels=256, out_channels=512, stride=2)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)  #(512, 11, 14)\n",
    "        out = out.view(out.size(0), 512, -1)\n",
    "        return out  \n",
    "\n",
    "# Instantiate the model\n",
    "model = ResNet()  # Assuming each layer has 2 blocks\n",
    "print('# paramters =', sum(p.numel() for p in model.parameters()))\n",
    "print('output size =', model(torch.randn((1,3,165,220))).size())\n",
    "print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# parameters = 171008\n",
      "output size = torch.Size([3, 512])\n",
      "tensor([[ 0.1027,  0.6116,  0.4241,  ...,  0.0612, -0.2628,  0.0431],\n",
      "        [-0.1940,  1.1913, -0.3832,  ..., -0.5313,  0.2483, -0.6485],\n",
      "        [ 0.4718,  0.7512, -0.5684,  ..., -0.5354, -1.1729,  0.9062]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "LanguageTransformer(\n",
      "  (embedding): Embedding(1370, 64)\n",
      "  (MHA): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "  )\n",
      "  (LN1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
      "  (ffn): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=256, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "  )\n",
      "  (LN2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
      "  (output_proj): Linear(in_features=64, out_features=512, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import hashlib\n",
    "\n",
    "class LanguageTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size=370, embed_size=64, extended_vocab_size=1000, transformer_heads=4, transformer_hidden=4*64):\n",
    "        super(LanguageTransformer, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.extended_vocab_size = extended_vocab_size\n",
    "        self.extra_embbeding = nn.Parameter(torch.randn(1, 1, embed_size))   # Learnable query embedding (similar to CLS token in BERT)\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size + extended_vocab_size, embed_size)\n",
    "        self.MHA = nn.MultiheadAttention(embed_size, transformer_heads)  #batch_first=True\n",
    "        self.LN1 = nn.LayerNorm(embed_size, eps=1e-06)\n",
    "        self.ffn = nn.Sequential(nn.Linear(embed_size, transformer_hidden), \n",
    "                    nn.GELU(),\n",
    "                    nn.Linear(transformer_hidden, embed_size))\n",
    "        self.LN2 = nn.LayerNorm(embed_size, eps=1e-06)\n",
    "        self.output_proj = nn.Linear(embed_size, 512)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        x = self.embedding(tokens)\n",
    "        x_extra = self.extra_embbeding.expand(tokens.size(0), tokens.size(1), -1)  # Un solo repetido embbeding o varios por cada token y cada batch?\n",
    "        x = self.LN1(x + self.MHA(query=x_extra, key=x, value=x)[0])\n",
    "        x = self.LN2(x + self.ffn(x))\n",
    "        final_output = self.output_proj(x[:,0,:])\n",
    "        return final_output\n",
    "\n",
    "    def hash_to_index(self, word):\n",
    "        \"\"\"Hash function to handle out-of-vocabulary words.\"\"\"\n",
    "        hash_digest = hashlib.sha256(word.encode()).digest()\n",
    "        hash_int = int.from_bytes(hash_digest, 'big')\n",
    "        # Reduce to integer between 370 and 1369\n",
    "        return 370 + (hash_int % (self.vocab_size + self.extended_vocab_size - self.vocab_size))\n",
    "    \n",
    "\n",
    "model = LanguageTransformer()\n",
    "print('# parameters =', sum(p.numel() for p in model.parameters()))\n",
    "tokens = torch.tensor([[1, 2, 34, 5], [4, 3, 2, 9], [6, 5, 9, 15]])\n",
    "output_embedding = model(tokens)\n",
    "print('output size =', output_embedding.shape)  # Should print: torch.Size([2, 512])\n",
    "print(output_embedding)\n",
    "print(model)\n",
    "# Example token indices (assuming preprocessing is already done)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# paramters = 25219072\n",
      "output size = torch.Size([10, 2, 512])\n",
      "TransformerEncoder(\n",
      "  (layers): ModuleList(\n",
      "    (0-7): 8 x TransformerEncoderLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "transformer_layer = TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "multimodal_transformer = TransformerEncoder(transformer_layer, num_layers=8)\n",
    "\n",
    "print('# paramters =', sum(p.numel() for p in multimodal_transformer.parameters()))\n",
    "print('output size =', multimodal_transformer(torch.randn((10,2,512))).size())\n",
    "print(multimodal_transformer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# paramters = 7086080\n",
      "output size = torch.Size([10, 512])\n",
      "ResidualLSTM(\n",
      "  (lstm1): LSTM(1536, 512, batch_first=True)\n",
      "  (Residualfc1): Linear(in_features=1536, out_features=512, bias=False)\n",
      "  (lstm2): LSTM(512, 512, batch_first=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Le hace control del gate de salida!!!!\n",
    "\n",
    "class ResidualLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=2):\n",
    "        super(ResidualLSTM, self).__init__() \n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_size, num_layers=1, batch_first=True)   \n",
    "        self.Residualfc1 = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.lstm2 = nn.LSTM(hidden_size, hidden_size, num_layers=1, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        o1, _ = self.lstm1(x)\n",
    "        o1 += self.Residualfc1(x)\n",
    "        o2, _ = self.lstm2(o1)\n",
    "        o2 += o1\n",
    "        return o2\n",
    "\n",
    "dual_lstm = ResidualLSTM(input_size=1536, hidden_size=512, num_layers=2)\n",
    "\n",
    "print('# paramters =', sum(p.numel() for p in dual_lstm.parameters()))\n",
    "print('output size =', dual_lstm(torch.randn((10,1536))).size())\n",
    "print(dual_lstm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ActionTypes.NONE\n",
      "ActionTypes.MOVE_COORDS\n",
      "ActionTypes.CLICK_COORDS\n",
      "ActionTypes.DBLCLICK_COORDS\n",
      "ActionTypes.MOUSEDOWN_COORDS\n",
      "ActionTypes.MOUSEUP_COORDS\n",
      "ActionTypes.SCROLL_UP_COORDS\n",
      "ActionTypes.SCROLL_DOWN_COORDS\n",
      "ActionTypes.CLICK_ELEMENT\n",
      "ActionTypes.PRESS_KEY\n",
      "ActionTypes.TYPE_TEXT\n",
      "ActionTypes.TYPE_FIELD\n",
      "ActionTypes.FOCUS_ELEMENT_AND_TYPE_TEXT\n",
      "ActionTypes.FOCUS_ELEMENT_AND_TYPE_FIELD\n",
      "{'done': False, 'env_reward': 0, 'raw_reward': 0, 'reason': None, 'root_dom': [1] body @ (0, 0) classes=[] children=1}\n",
      "utterance Click button ONE.\n",
      "dom_elements ({'ref': 1, 'parent': 0, 'left': array([0.], dtype=float32), 'top': array([0.], dtype=float32), 'width': array([500.], dtype=float32), 'height': array([210.], dtype=float32), 'tag': 'body', 'text': '', 'value': '', 'id': '', 'classes': '', 'bg_color': array([0.33333334, 0.33333334, 0.33333334, 1.        ], dtype=float32), 'fg_color': array([0., 0., 0., 1.], dtype=float32), 'flags': array([1, 0, 0, 0], dtype=int8)}, {'ref': 2, 'parent': 1, 'left': array([0.], dtype=float32), 'top': array([0.], dtype=float32), 'width': array([160.], dtype=float32), 'height': array([210.], dtype=float32), 'tag': 'div', 'text': '', 'value': '', 'id': 'wrap', 'classes': '', 'bg_color': array([1., 1., 1., 1.], dtype=float32), 'fg_color': array([0., 0., 0., 1.], dtype=float32), 'flags': array([0, 0, 0, 0], dtype=int8)}, {'ref': 3, 'parent': 2, 'left': array([0.], dtype=float32), 'top': array([50.], dtype=float32), 'width': array([160.], dtype=float32), 'height': array([4.], dtype=float32), 'tag': 'div', 'text': '', 'value': '', 'id': 'area', 'classes': '', 'bg_color': array([0., 0., 0., 0.], dtype=float32), 'fg_color': array([0., 0., 0., 1.], dtype=float32), 'flags': array([0, 0, 0, 0], dtype=int8)}, {'ref': 4, 'parent': 3, 'left': array([99.], dtype=float32), 'top': array([115.], dtype=float32), 'width': array([40.], dtype=float32), 'height': array([40.], dtype=float32), 'tag': 'button', 'text': 'ONE', 'value': '', 'id': 'subbtn', 'classes': '', 'bg_color': array([0.9372549, 0.9372549, 0.9372549, 1.       ], dtype=float32), 'fg_color': array([0., 0., 0., 1.], dtype=float32), 'flags': array([0, 0, 0, 1], dtype=int8)}, {'ref': 5, 'parent': 3, 'left': array([101.], dtype=float32), 'top': array([116.], dtype=float32), 'width': array([40.], dtype=float32), 'height': array([40.], dtype=float32), 'tag': 'button', 'text': 'TWO', 'value': '', 'id': 'subbtn2', 'classes': '', 'bg_color': array([0.9372549, 0.9372549, 0.9372549, 1.       ], dtype=float32), 'fg_color': array([0., 0., 0., 1.], dtype=float32), 'flags': array([0, 0, 0, 1], dtype=int8)})\n",
      "screenshot (210, 160, 3)\n",
      "fields (('target', 'ONE'),)\n",
      "action OrderedDict({'action_type': 8, 'coords': array([ 75.01943, 153.05673], dtype=float32), 'field': 15, 'key': 84, 'ref': 4, 'text': 'ICd'})\n",
      "0.5789 True False {'done': True, 'env_reward': 0.5789, 'raw_reward': 1, 'reason': None, 'elapsed': 4.212838172912598}\n",
      "utterance \n",
      "dom_elements ()\n",
      "screenshot (210, 160, 3)\n",
      "fields ()\n",
      "0.5789\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import gymnasium\n",
    "import miniwob\n",
    "from miniwob.action import ActionTypes, ActionSpaceConfig\n",
    "import numpy as np \n",
    "#https://github.com/Farama-Foundation/miniwob-plusplus\n",
    "# https://miniwob.farama.org/\n",
    "gymnasium.register_envs(miniwob)\n",
    "\n",
    "env = gymnasium.make('miniwob/click-test-2-v1', render_mode='human', action_space_config = \"all_supported\")\n",
    "# \"humphreys22\"  \"all_supported\", \"shi17\", \"liu18\"\n",
    "for x in iter(ActionTypes):\n",
    "    print(x)\n",
    "\n",
    "# Wrap the code in try-finally to ensure proper cleanup.\n",
    "try:\n",
    "  # Start a new episode.\n",
    "  obs, info = env.reset()\n",
    "  print(info)\n",
    "  for key in obs.keys():\n",
    "      xx = obs[key] if not isinstance(obs[key], np.ndarray) else obs[key].shape\n",
    "      print(key, xx)\n",
    "\n",
    "  assert obs[\"utterance\"] == \"Click button ONE.\"\n",
    "  assert obs[\"fields\"] == ((\"target\", \"ONE\"),)\n",
    "  time.sleep(4)       # Only here to let you look at the environment.\n",
    "  \n",
    "  \n",
    "  # Find the HTML element with text \"ONE\".\n",
    "  for element in obs[\"dom_elements\"]:\n",
    "    if element[\"text\"] == \"ONE\":\n",
    "      break\n",
    "\n",
    "  action = env.unwrapped.create_action(ActionTypes.CLICK_ELEMENT, ref=element[\"ref\"])\n",
    "  # NONE, MOVE_COORDS, CLICK_COORDS, DBLCLICK_COORDS, MOUSEDOWN_COORDS, MOUSEUP_COORDS, SCROLL_UP_COORDS, SCROLL_DOWN_COORDS, PRESS_KEY, TYPE_TEXT,\n",
    "  # CLICK_ELEMENT, TYPE_FIELD, FOCUS_ELEMENT_AND_TYPE_TEXT, FOCUS_ELEMENT_AND_TYPE_FIELD\n",
    "  print('action', action)\n",
    "  obs, reward, terminated, truncated, info = env.step(action)\n",
    "  print(reward, terminated, truncated, info)\n",
    "  for key in obs.keys():\n",
    "      xx = obs[key] if not isinstance(obs[key], np.ndarray) else obs[key].shape\n",
    "      print(key, xx)\n",
    "\n",
    "  # Check if the action was correct. \n",
    "  print(reward)      # Should be around 0.8 since 2 seconds has passed.\n",
    "  assert terminated is True\n",
    "  time.sleep(2)\n",
    "\n",
    "finally:\n",
    "  env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "import miniwob\n",
    "gymnasium.register_envs(miniwob)\n",
    "env = gymnasium.make('miniwob/click-test-2-v1', render_mode='human')\n",
    "try:\n",
    "  observation, info = env.reset(seed=42)\n",
    "  for _ in range(1000):\n",
    "    action = policy(observation)  # User-defined policy function\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    if terminated:\n",
    "      observation, info = env.reset()\n",
    "finally:\n",
    "  env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet18\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class CCNet(nn.Module):\n",
    "    def __init__(self, num_tokens, num_actions, num_cursor, num_keys, num_fields, embed_dim, num_heads, hidden_dim):\n",
    "        super(CCNet, self).__init__()\n",
    "        \n",
    "        # Image processing path\n",
    "        self.resnet_blocks = nn.Sequential(*list(resnet18(pretrained=True).children())[:-2])\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Language processing path\n",
    "        self.embedding = nn.Embedding(num_tokens, embed_dim)\n",
    "        transformer_layer = TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads)\n",
    "        self.language_transformer = TransformerEncoder(transformer_layer, num_layers=1)\n",
    "        \n",
    "        # Multimodal Transformer\n",
    "        self.multimodal_transformer = TransformerEncoder(transformer_layer, num_layers=8)\n",
    "        \n",
    "        # LSTM for integrating sequence information\n",
    "        self.lstm = nn.LSTM(input_size=embed_dim, hidden_size=hidden_dim, num_layers=2)\n",
    "        \n",
    "        # Output heads\n",
    "        self.action_type_head = nn.Linear(hidden_dim, num_actions)\n",
    "        self.cursor_head = nn.Linear(hidden_dim, num_cursor*2)  # x and y coordinates\n",
    "        self.key_index_head = nn.Linear(hidden_dim, num_keys)\n",
    "        \n",
    "        # Attention for task field index\n",
    "        self.task_field_attention = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "        self.query_embed = nn.Parameter(torch.rand(embed_dim))\n",
    "        \n",
    "    def forward(self, images, token_indices, previous_actions):\n",
    "        # Process images through ResNet and flatten\n",
    "        image_features = self.flatten(self.resnet_blocks(images))\n",
    "        \n",
    "        # Process language tokens through embedding and transformer\n",
    "        token_embeddings = self.embedding(token_indices)\n",
    "        language_features = self.language_transformer(token_embeddings)\n",
    "        \n",
    "        # Concatenate processed image and language features\n",
    "        multimodal_features = torch.cat((image_features, language_features), dim=1)\n",
    "        \n",
    "        # Process multimodal features through transformer\n",
    "        multimodal_features = self.multimodal_transformer(multimodal_features)\n",
    "        \n",
    "        # Prepare for LSTM\n",
    "        prev_action_embedding = self.embedding(previous_actions)\n",
    "        lstm_input = torch.cat((multimodal_features, prev_action_embedding.unsqueeze(0)), dim=2)\n",
    "        \n",
    "        # LSTM output\n",
    "        _, (hidden, _) = self.lstm(lstm_input)\n",
    "        \n",
    "        # Action type prediction\n",
    "        action_type = self.action_type_head(hidden[-1])\n",
    "        \n",
    "        # Cursor coordinates prediction\n",
    "        cursor_coords = self.cursor_head(hidden[-1]).view(-1, 2)  # Reshape to (x, y)\n",
    "        \n",
    "        # Keyboard key index prediction\n",
    "        key_index = self.key_index_head(hidden[-1])\n",
    "        \n",
    "        # Task field index prediction using attention\n",
    "        query = self.query_embed.unsqueeze(0).expand(token_indices.size(0), -1, -1)\n",
    "        task_field_logits, _ = self.task_field_attention(query, token_embeddings, token_embeddings)\n",
    "        \n",
    "        return action_type, cursor_coords, key_index, task_field_logits\n",
    "\n",
    "# Example usage\n",
    "num_tokens = 1370  # Your vocab size + additional OOV tokens\n",
    "num_actions = 10   # Number of possible actions (from your architecture)\n",
    "num_cursor = 51    # Assuming this is the number of bins for cursor coordinates\n",
    "num_keys = 512     # Assuming this is the number of possible keys (this might vary)\n",
    "num_fields = 512   # For task fields\n",
    "embed_dim = 64     # Embedding dimension from your architecture\n",
    "num_heads = 4      # Number of heads in multi-head attention mechanisms\n",
    "hidden_dim = 512   # Hidden dimension size for LSTMs\n",
    "\n",
    "model = CCNet(num_tokens, num_actions, num_cursor, num_keys, num_fields, embed_dim, num_heads, hidden_dim)\n",
    "\n",
    "# Create dummy data for testing\n",
    "dummy_images = torch.randn\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccnet-uv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
