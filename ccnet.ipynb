{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent receives visual inputs (165x220\n",
    "RGB pixels) and language inputs (an example input is shown\n",
    "in Appendix Fig. 9). The pixel inputs pass through a series\n",
    "of four ResNet blocks, with 3×3 kernels, strides of 2, 2, 2,\n",
    "2, and an increasing number of output channels (32, 128,\n",
    "256, 512). This results in 14×11 feature vectors, which we\n",
    "flatten into a list of 154 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# paramters = 4791680\n",
      "output size = torch.Size([1, 512, 154])\n",
      "ResNet(\n",
      "  (layer1): ResBlock(\n",
      "    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential(\n",
      "      (0): Conv2d(3, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): ResBlock(\n",
      "    (conv1): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential(\n",
      "      (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): ResBlock(\n",
      "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): ResBlock(\n",
      "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.shortcut = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                                      nn.BatchNorm2d(out_channels),\n",
    "                                      )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.layer1 = ResBlock(in_channels=3,   out_channels=32,  stride=2)  \n",
    "        self.layer2 = ResBlock(in_channels=32,  out_channels=128, stride=2)  \n",
    "        self.layer3 = ResBlock(in_channels=128, out_channels=256, stride=2)  \n",
    "        self.layer4 = ResBlock(in_channels=256, out_channels=512, stride=2)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)  #(512, 11, 14)\n",
    "        out = out.view(out.size(0), 512, -1)\n",
    "        return out  \n",
    "\n",
    "# Instantiate the model\n",
    "model = ResNet()  # Assuming each layer has 2 blocks\n",
    "print('# paramters =', sum(p.numel() for p in model.parameters()))\n",
    "print('output size =', model(torch.randn((1,3,165,220))).size())\n",
    "print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# paramters = 171008\n",
      "output size = torch.Size([2, 512])\n",
      "LanguageTransformer(\n",
      "  (embedding): Embedding(1370, 64)\n",
      "  (transformer): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_proj): Linear(in_features=64, out_features=512, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import numpy as np\n",
    "import hashlib\n",
    "\n",
    "\n",
    "class LanguageTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size=370, embed_size=64, extended_vocab_size=1000, transformer_heads=4, transformer_hidden=256):\n",
    "        super(LanguageTransformer, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.extended_vocab_size = extended_vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size + extended_vocab_size, embed_size)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_size))   # Learnable query embedding (similar to CLS token in BERT)\n",
    "        self.transformer = TransformerEncoder(TransformerEncoderLayer(d_model=embed_size, nhead=transformer_heads, dim_feedforward=transformer_hidden),num_layers=1)\n",
    "        self.output_proj = nn.Linear(embed_size, 512)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        token_embeddings = self.embedding(tokens)\n",
    "        cls_tokens = self.cls_token.expand(-1, tokens.size(0), -1).permute(1, 0, 2)\n",
    "        combined = torch.cat((cls_tokens, token_embeddings), dim=1)\n",
    "        transformed = self.transformer(combined)\n",
    "        cls_output = transformed[:, 0, :]\n",
    "        final_output = self.output_proj(cls_output)\n",
    "        return final_output\n",
    "\n",
    "    def hash_to_index(self, word):\n",
    "        \"\"\"Hash function to handle out-of-vocabulary words.\"\"\"\n",
    "        hash_digest = hashlib.sha256(word.encode()).digest()\n",
    "        hash_int = int.from_bytes(hash_digest, 'big')\n",
    "        # Reduce to integer between 370 and 1369\n",
    "        return 370 + (hash_int % (self.vocab_size + self.extended_vocab_size - self.vocab_size))\n",
    "    \n",
    "\n",
    "model = LanguageTransformer()\n",
    "print('# paramters =', sum(p.numel() for p in model.parameters()))\n",
    "tokens = torch.tensor([[1, 2, 34, 5], [4, 3, 2, 9]])\n",
    "output_embedding = model(tokens)\n",
    "print('output size =', output_embedding.shape)  # Should print: torch.Size([2, 512])\n",
    "print(model)\n",
    "# Example token indices (assuming preprocessing is already done)\n",
    "\n",
    "# Dudas\n",
    "# porque transformer_hidden=256 ¿64*4 = 256?\n",
    "# Creo que necesito crear mi propia impmentacion del tranformerencoder porque esta no consider cross-atenteiotn \n",
    "# No creo que la salida solo tengamos un solo vector 512, deberia ser un vector de 512 por cda token en el string? \n",
    "# ¿omo son los datos de texto? Miniwob++\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet18\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class CCNet(nn.Module):\n",
    "    def __init__(self, num_tokens, num_actions, num_cursor, num_keys, num_fields, embed_dim, num_heads, hidden_dim):\n",
    "        super(CCNet, self).__init__()\n",
    "        \n",
    "        # Image processing path\n",
    "        self.resnet_blocks = nn.Sequential(*list(resnet18(pretrained=True).children())[:-2])\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Language processing path\n",
    "        self.embedding = nn.Embedding(num_tokens, embed_dim)\n",
    "        transformer_layer = TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads)\n",
    "        self.language_transformer = TransformerEncoder(transformer_layer, num_layers=1)\n",
    "        \n",
    "        # Multimodal Transformer\n",
    "        self.multimodal_transformer = TransformerEncoder(transformer_layer, num_layers=8)\n",
    "        \n",
    "        # LSTM for integrating sequence information\n",
    "        self.lstm = nn.LSTM(input_size=embed_dim, hidden_size=hidden_dim, num_layers=2)\n",
    "        \n",
    "        # Output heads\n",
    "        self.action_type_head = nn.Linear(hidden_dim, num_actions)\n",
    "        self.cursor_head = nn.Linear(hidden_dim, num_cursor*2)  # x and y coordinates\n",
    "        self.key_index_head = nn.Linear(hidden_dim, num_keys)\n",
    "        \n",
    "        # Attention for task field index\n",
    "        self.task_field_attention = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "        self.query_embed = nn.Parameter(torch.rand(embed_dim))\n",
    "        \n",
    "    def forward(self, images, token_indices, previous_actions):\n",
    "        # Process images through ResNet and flatten\n",
    "        image_features = self.flatten(self.resnet_blocks(images))\n",
    "        \n",
    "        # Process language tokens through embedding and transformer\n",
    "        token_embeddings = self.embedding(token_indices)\n",
    "        language_features = self.language_transformer(token_embeddings)\n",
    "        \n",
    "        # Concatenate processed image and language features\n",
    "        multimodal_features = torch.cat((image_features, language_features), dim=1)\n",
    "        \n",
    "        # Process multimodal features through transformer\n",
    "        multimodal_features = self.multimodal_transformer(multimodal_features)\n",
    "        \n",
    "        # Prepare for LSTM\n",
    "        prev_action_embedding = self.embedding(previous_actions)\n",
    "        lstm_input = torch.cat((multimodal_features, prev_action_embedding.unsqueeze(0)), dim=2)\n",
    "        \n",
    "        # LSTM output\n",
    "        _, (hidden, _) = self.lstm(lstm_input)\n",
    "        \n",
    "        # Action type prediction\n",
    "        action_type = self.action_type_head(hidden[-1])\n",
    "        \n",
    "        # Cursor coordinates prediction\n",
    "        cursor_coords = self.cursor_head(hidden[-1]).view(-1, 2)  # Reshape to (x, y)\n",
    "        \n",
    "        # Keyboard key index prediction\n",
    "        key_index = self.key_index_head(hidden[-1])\n",
    "        \n",
    "        # Task field index prediction using attention\n",
    "        query = self.query_embed.unsqueeze(0).expand(token_indices.size(0), -1, -1)\n",
    "        task_field_logits, _ = self.task_field_attention(query, token_embeddings, token_embeddings)\n",
    "        \n",
    "        return action_type, cursor_coords, key_index, task_field_logits\n",
    "\n",
    "# Example usage\n",
    "num_tokens = 1370  # Your vocab size + additional OOV tokens\n",
    "num_actions = 10   # Number of possible actions (from your architecture)\n",
    "num_cursor = 51    # Assuming this is the number of bins for cursor coordinates\n",
    "num_keys = 512     # Assuming this is the number of possible keys (this might vary)\n",
    "num_fields = 512   # For task fields\n",
    "embed_dim = 64     # Embedding dimension from your architecture\n",
    "num_heads = 4      # Number of heads in multi-head attention mechanisms\n",
    "hidden_dim = 512   # Hidden dimension size for LSTMs\n",
    "\n",
    "model = CCNet(num_tokens, num_actions, num_cursor, num_keys, num_fields, embed_dim, num_heads, hidden_dim)\n",
    "\n",
    "# Create dummy data for testing\n",
    "dummy_images = torch.randn\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccnet-uv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
